---
title: "PML_Final_Project"
author: "Viswathish Dinesh"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This project aims to use accelerometer data to predict the class of exercise performed. The training and test data have been provided, with the training data containing the "classe" variable, which is what needs to be predicted in the test dataset.

# Pre Analysis

Before any analysis can begin, we need to complete some cursory steps to get our environment and our data ready.

## Load Libraries

The following package must be loaded for this analysis to create and test the model fit we will train on the training data.

```{r libraries, include=FALSE}
## Output has been muted to prevent unnecessary text
library(caret)
```

## Load Data

The data will be loaded into R. The training data can be found [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv). The test data can be found [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv). This data was downloaded on August 25, 2025.

```{r import data, cache = TRUE}
training <- read.csv("data/pml-training.csv")
testing <- read.csv("data/pml-testing.csv")
```

## Data Cleaning

There are many unnecessary columns and variables included in both data sets. Some of these columns contain almost no data (a very large number of NA values), whilst other columns are descriptors for the data point itself, rather than the accelerometer readings that we are interested in using. There are also columns that have near-zero variance, indicating that they are near constant values, which is not useful for prediction, and can introduce unwanted artifacts to the training models.

Furthermore, the "classe" variable was read in as a "character" variable. To ensure proper prediction, we will convert this column to a "factor" variable.

```{r data cleaning}
## Remove features with near-zero variance
NZV <- nearZeroVar(training)

training <- training[,-NZV]
testing <- testing[,-NZV]

## Remove features not contributing to accelerometer readings

col_rm <- grepl("^X|timestamp|user_name", names(training))
training <- training[, !col_rm]
testing <- testing[, !col_rm]

## Remove columns with a majority of missing values

na_cols <- sapply(training, function(x) mean(is.na(x))) > 0.95
training <- training[,!na_cols]
testing <- testing[,!na_cols]

## Set "classe" feature as factor
training$classe <- factor(training$classe)
```

# Creating the model

Our environment and our data is now ready for processing and prediction.

## Split the data into training and validation sets

Before training the model on the training data set, we must split it into training and validation data sets. The validation set will allow us to test the accuracy of the model on unseen data. We will split the data 70/30 for training against validation.

```{r training v validation}
set.seed(12345) # Set seed for reproducibility
inTrain <- createDataPartition(training$classe, p = 0.70, list = FALSE)
train_data <- training[inTrain,]
valid_data <- training[-inTrain,]
```

## Create the model on the training data

The model will be created using the random forest method, as it has been shown to be a very robust prediction method with high accuracy. The "trainControl" parameter will allow us to perform 5-fold cross validation to further improve accuracy.

```{r model creation, cache=TRUE}
set.seed(12345) ## Set seed for reproducibility
## Add training method for cross validation
train_control <- trainControl(method = "cv", number = 5)
modelFit <- train(classe ~ ., data = train_data, method = "rf", ntree = 250, trControl = train_control)
```

## Test the model on the validation data

Now that our model has been created, we can predict the "classe" variable of the validation data set with it. A confusion matrix has been used to cross-check the predicted outcomes against the actual outcomes.

```{r validation, cache = TRUE}
set.seed(12345) ## Set seed for reproducibility
## Predicting values using model
predictValid <- predict(modelFit, newdata = valid_data)
## Testing accuracy of model
con_matrix <- confusionMatrix(predictValid, valid_data$classe)
con_matrix
```

The accuracy of this model on the validation data is `r con_matrix$overall[1]`. The estimated out-of-sample error is `r 1-con_matrix$overall[1]`.

# Predicting outcomes of testing data

Since our model seems to be highly accurate, we can now use the same model to predict the outcomes of the testing data using the same method we used for predicting the validation data. The outcomes have been printed below the code chunk.

```{r model on test data, cache=TRUE}
set.seed(12345) ## Set seed for reproducibility
## Predict values using model
predictTest <- predict(modelFit, newdata = testing)
predictTest
```

The true values of the "classe" outcome for the testing data can be found in the "Course Project Prediction Quiz". After entering the predicted value for each test case into the quiz, the outcome showed that the model predicted 100% of the values correctly. Therefore, the out-of-sample error rate is 0%, even less so than what was seen in the validation step. Typically, this error rate is expected to be higher on unseen test data than in validation data. This discrepancy can likely be attributed to the small sample size of the test data. 